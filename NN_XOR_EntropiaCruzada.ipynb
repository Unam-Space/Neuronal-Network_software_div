{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import json, matplotlib\n",
    "s = json.load( open(\"styles/bmh_matplotlibrc.json\") )\n",
    "matplotlib.rcParams.update(s)\n",
    "from IPython.core.pylabtools import figsize\n",
    "figsize(11, 5)\n",
    "colores = [\"#348ABD\", \"#A60628\",\"#06A628\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de activación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def logistica(z):\n",
    "    \"\"\"\n",
    "    Devuelve la función logística evaluada componente por componente\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f205cb7b8b0b4e80a1eabefaa2970b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Output(),), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "\n",
    "@interact\n",
    "def plot_log():\n",
    "    z = np.arange(-5,5,0.1)\n",
    "    figure(figsize=(4,2))\n",
    "    plt.plot(z, logistica(z))\n",
    "    plt.xlabel(\"$z$\")\n",
    "    plt.ylabel(\"$\\sigma$\")\n",
    "    plt.title(\"$\\sigma = \\\\frac{1}{1 + e^{-z}}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def derivada_logistica(z):\n",
    "    \"\"\"\n",
    "    Función que, dado un arreglo de valores z\n",
    "    calcula el valor de la derivada para cada entrada.\n",
    "    \"\"\"\n",
    "    g = logistica(z)\n",
    "    return g * (1 - g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c402dc07692b4cdca107c510aa226e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Output(),), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def plot_logp():\n",
    "    z = np.arange(-5,5,0.1)\n",
    "    figure(figsize=(4,2))\n",
    "    plt.plot(z, derivada_logistica(z))\n",
    "    plt.xlabel(\"$z$\")\n",
    "    plt.ylabel(\"$\\sigma'$\")\n",
    "    plt.title(\"$\\sigma' = \\sigma (1-\\sigma)$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def derivada_logistica_atajo(val_sigma):\n",
    "    \"\"\"\n",
    "    Función que, dado un arreglo de valores de sigma(z)\n",
    "    calcula el valor de la derivada para cada entrada.\n",
    "    \n",
    "    Si ya se cuenta con esos valores, es más eficiente\n",
    "    calcular esto directamente.\n",
    "    \"\"\"\n",
    "    logistics = np.vectorize(logistica)\n",
    "    return logistics(val_sigma)*(1-logistics(val_sigma))\n",
    "    ## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51a349950c54bf9b3c34c49f381dd4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Output(),), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def plot_logpa():\n",
    "    z = np.arange(-5,5,0.1)\n",
    "    val = logistica(z)\n",
    "    figure(figsize=(4,2))\n",
    "    plt.plot(z, derivada_logistica_atajo(val))\n",
    "    plt.xlabel(\"$z$\")\n",
    "    plt.ylabel(\"$\\sigma'$\")\n",
    "    plt.title(\"$\\sigma' = \\sigma (1-\\sigma)$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de error: Entropía cruzada\n",
    "\n",
    "La función de error más sencilla utilizada para problemas de clasificación es la **entropía cruzada**\n",
    "\\begin{align}\n",
    "  J(\\Theta) =& - \\frac{1}{m} \\left[ \\sum_{i=1}^m \\sum_{k=1}^K   y_k^{(i)} \\log(h_\\Theta(x^{(i)}))_k  + (1 - y_k^{(i)}) \\log(1 - h_\\Theta(x^{(i)}))_k   \\right]    +  \\\\\n",
    "  & \\frac{\\lambda}{2m} \\sum_{l=1}^{L-1} \\sum_{i=1}^{s_L} \\sum_{j=1}^{s_{l+1}} (\\theta_{ji}^{(l)})^2\n",
    "\\end{align}\n",
    "donde:\n",
    "  * $K$ es el número de neuronas de salida.\n",
    "  * $s_l$ es el número de neuronas en la capa $l$.\n",
    "  * $m$ es el número de ejemplares de entrenamiento.\n",
    "  \n",
    "En su forma vectorizada, las componentes del gradiente están dadas por:\n",
    "\\begin{align}\n",
    "  \\delta^{(L)} &= (A^{(L)})^T - Y  \\\\\n",
    "  \\delta^{(L-1)} &= \\delta^{(L)} \\Theta_{[:,1:]}^{(L-1)} \\circ g'(z^{(L-1)}) \\\\\n",
    "  ... \\\\\n",
    "  \\delta^{(1)} &= \\delta^{(0)} \\Theta_{[:,1:]}^{(1)} \\circ g'(z^{(1)}) \\\\\n",
    "\\end{align}\n",
    "con:\n",
    "\\begin{align}\n",
    "  g'(z^{(l)}) = A^{(l)} \\circ (1 - A^{(l)})\n",
    "\\end{align}\n",
    "en general:\n",
    "\\begin{align}\n",
    "  \\Delta^{(l)} =& (\\delta^{(l+1)})^T A^{(l)}   &   \\nabla^{(l)} =& \\frac{1}{m}\\Delta^{(l)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cross_entropy(y, h):\n",
    "    return - y * np.log(h) - (1 - y) * np.log(1 - h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de022053bcb4c9b8ff3c62917fbd5ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Output(),), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def plot_crossentropy():\n",
    "    \"\"\"\n",
    "    Muestra como, en el rango donde está definida, la entropía cruzada tiene\n",
    "    sus valores más pequeños donde 'y' y 'h' coinciden, y su valor más alto\n",
    "    donde éstos son opuestos.\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    #ax = fig.gca(projection='3d') modified \n",
    "    # Datos\n",
    "    X = np.arange(0.01, 1, 0.05)\n",
    "    Y = np.arange(0.01, 1, 0.05)\n",
    "    X, Y = np.meshgrid(X, Y)\n",
    "    CE = cross_entropy(X, Y)\n",
    "    surf = ax.plot_surface(X, Y, CE, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "    ax.set_xlabel(\"$y$\")\n",
    "    ax.set_ylabel(\"$h$\")\n",
    "    ax.set_title(\"Entropía cruzada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red neuronal\n",
    "La red implementa encadenamiento hacia adelante (para evaluar) y hacia atrás (para entrenarse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cuando se programa un algoritmo que depende de la generación de números aleatorios\n",
    "# es buena costumbre fijar la semilla a partir de la cual dichos números son generados.\n",
    "# De este modo el comportamiento sobre los datos de entrada será predecible mientras\n",
    "# se está desarrollando/probando el código.\n",
    "#\n",
    "# Una vez el código está listo, se debe usar variando los valores de esta semilla\n",
    "# para ver los efectos de la aleatoriedad simulada.\n",
    "\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Multicapa:\n",
    "    \"\"\"\n",
    "    Red neuronal con tres capas:\n",
    "    \n",
    "    Entrada\n",
    "    Oculta\n",
    "    Salida\n",
    "    \n",
    "    Los parámetros (pesos) que conectan las capas se encuentran en matrices\n",
    "    con los nombres siguientes:\n",
    "    \n",
    "    Entrada -> self.Theta_0 -> Oculta\n",
    "    Oculta  -> self.Theta_1 -> Salida\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_entrada, n_ocultas, n_salidas):\n",
    "        \"\"\"\n",
    "        Inicializa la red neuronal, con pesos Theta_0 y Theta_1 aleatorios,\n",
    "        esta implementación debe incluir el uso de sesgos, por lo que éstos\n",
    "        no se cuentan en los parámetros siguientes, puedes incluirlos como\n",
    "        neuronas extra o en sus propias matrices, sólo sé consistente pues\n",
    "        esto afectará tu implementación.\n",
    "        \n",
    "        :param n_entrada: número de datos de entrada (sin contar el sesgo)\n",
    "        :param n_ocultas: número de neuronas ocultas\n",
    "        :param n_salidas: número de nueronas de salida\n",
    "        \"\"\"\n",
    "        ## TODO\n",
    "        self.n_entrada=n_entrada\n",
    "        self.n_ocultas=n_ocultas\n",
    "        self.n_salidas=n_salidas\n",
    "\n",
    "        ##Pesos aleatorios\n",
    "        self.Theta_0=np.random.randn(self.n_ocultas,self.n_entrada)\n",
    "        self.Theta_1=np.random.randn(self.n_ocultas,self.n_salidas)\n",
    "        self.error=0\n",
    "        self.Grad_0=np.zeros((self.n_entrada,self.n_ocultas))\n",
    "        self.Grad_1=np.zeros((self.n_ocultas,self.n_salidas))\n",
    "\n",
    "\n",
    "    def feed_forward(self, X, vector = None):\n",
    "        \"\"\" Calcula las salidas, dados los datos de entrada en forma de matriz.\n",
    "        Guarda los parámetros siguientes:\n",
    "        A0: activaciones de la capa de entrada, ya con sesgos\n",
    "        Z1: potenciales de la capa oculta, aún sin sesgo\n",
    "        A1: activaciones de la capa oculta, ya con sesgos\n",
    "        Z2: potencales de la capa de salida\n",
    "        A2: activaciones de la capa de salida\n",
    "        \n",
    "        :param vector: [opcional] se utilizarán los pesos indicados en este\n",
    "                       vector en lugar de los pesos actuales de la red.\n",
    "        \"\"\"\n",
    "        #Colocamos 1's en las primeras columnas\n",
    "        A0 = np.insert(X, 0, 1, axis=1)\n",
    "        i,j=A0.shape\n",
    "        if vector is None:\n",
    "            # Obtenemos los pesos actuales de la red\n",
    "            Theta_0 = self.Theta_0\n",
    "            Theta_1 = self.Theta_1\n",
    "        else:\n",
    "            # Obtenemos los pesos del vector dado\n",
    "            Theta_0,Theta_1 = self.reconstruct_matrices(vector)\n",
    "            print(\"there's vector\")\n",
    "\n",
    "            # Calculamos la capa oculta\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        self.Theta_0=np.random.randn(j,self.n_ocultas)\n",
    "        \"\"\"\n",
    "        @Daniel\n",
    "        \n",
    "        En esta parte creo no se debe ejecutar siempre la linea anterior\n",
    "        dado que al actualizar las thetas, las volvemos a crear cada\n",
    "        que se le da una entrada a la red.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        Theta_0=self.Theta_0\n",
    "        print(\"Theta0-2\", Theta_0)\n",
    "        Z1 = np.dot(A0, Theta_0)\n",
    "        A1 = logistica(Z1)\n",
    "        A1 = np.insert(A1, 0, 1, axis=1)  # Agregamos el sesgo a la capa oculta\n",
    "        #Agrega bias a theta1\n",
    "        Theta_1=np.random.randn(A1.shape[1],self.n_salidas)\n",
    "        #Theta_1=np.insert(Theta_1,0, np.random.randn(), axis=0)#Agregando sesgo\n",
    "        \n",
    "        #print(\"T2\",Theta_1)\n",
    "        #print(\"A1\", A1)\n",
    "        # Calculamos la capa de salida\n",
    "        Z2 = np.dot(A1, Theta_1)\n",
    "        A2 = logistica(Z2)\n",
    "\n",
    "\n",
    "        # Guardamos los resultados en atributos de la red\n",
    "        self.A0 = A0\n",
    "        self.Z1 = Z1\n",
    "        self.A1 = A1\n",
    "        self.Z2 = Z2\n",
    "        self.A2 = A2\n",
    "\n",
    "\n",
    "    def back_propagate(self, X, Y, lambda_r = 0.0):\n",
    "        \"\"\" Calcula el error y su gradiente dados los pesos actuales de la red\n",
    "        y los resultados esperados.\n",
    "        \n",
    "        Guarda el error en el atributo self.error y el gradiente en matrices\n",
    "        self.Grad_1 y self.Grad_0, que tienen la misma forma de Theta_0 y Theta_1.\n",
    "        \n",
    "        :param X: matriz de entradas\n",
    "        :param Y: matriz de salidas deseadas\n",
    "        :param lambda_r: coeficiente de regularización\n",
    "        \"\"\"\n",
    "        ## TODO\n",
    "           # Forward pass\n",
    "        self.feed_forward(X)\n",
    "    \n",
    "        # Error en la capa de salida\n",
    "        delta2 = self.A2 - Y\n",
    "    \n",
    "        # Potenciales de la capa oculta, incluyendo el sesgo\n",
    "        Z1_with_bias = np.insert(self.Z1, 0, 1, axis=1)\n",
    "    \n",
    "        self.Grad_1 = Z1_with_bias.T @ delta2 + lambda_r * np.insert(self.Theta_1[1:], 0, 0, axis=0)\n",
    "    \n",
    "        # Error en la capa oculta\n",
    "        delta1 = delta2 @ self.Theta_1.T * derivada_logistica_atajo(Z1_with_bias)\n",
    "        delta1 = delta1[:, 1:]\n",
    "    \n",
    "        # Entradas de la red, incluyendo el sesgo\n",
    "        A0_with_bias = np.insert(self.A0, 0, 1, axis=1)\n",
    "    \n",
    "        # Gradiente de Theta_0 con regularización\n",
    "        self.Grad_0 = A0_with_bias.T @ delta1 + lambda_r * np.insert(self.Theta_0[1:], 0, 0, axis=0)\n",
    "    \n",
    "        # Error de la red con regularización\n",
    "        self.error = (-Y * np.log(self.A2) - (1 - Y) * np.log(1 - self.A2)).mean() \\\n",
    "                 + (lambda_r / 2) * (np.sum(self.Theta_0[1:] ** 2) + np.sum(self.Theta_1[1:] ** 2))\n",
    "\n",
    "        \n",
    "    def calc_error(self, X, Y, vector=None, lambda_r = 0.0):\n",
    "        \"\"\"\n",
    "        Calcula el error que se cometería utilizando los pesos en 'vector' en lugar\n",
    "        de los pesos actuales de la red.\n",
    "        \n",
    "        :returns: el error\n",
    "        \"\"\"\n",
    "        ## TODO\n",
    "        #A0,A1,A2,Z1,Z2=self.feed_forward(X,vector=vector)\n",
    "        #self.agregar_elemento_a_cada_renglon(Y,1)\n",
    "        #error=np.sum((A2-Y)**2)/X.shape[1]\n",
    "        self.feed_forward(X, vector=vector)\n",
    "        myA2 = np.insert(self.A2, 0, 1, axis=1)\n",
    "        print(\"a2 de error\",myA2)\n",
    "        print(\"original A2\",self.A2)\n",
    "        m=Y.shape[0]\n",
    "        \n",
    "        \n",
    "        \n",
    "        J=(-1/m)*np.sum(Y*np.log(myA2) + (1-Y)*np.log(1-myA2))\n",
    "        if lambda_r>0:\n",
    "            reg_term=(lambda_r / (2*m)) * (np.sum(np.square(self.Theta_0)) + np.sum(np.square(self.Theta_1)))\n",
    "            J+=reg_term\n",
    "        return J\n",
    "\n",
    "\n",
    "    def vector_weights(self):\n",
    "        \"\"\"\n",
    "        Acomoda a todos los parámetros en las matrices de sesgos y pesos, en un solo vector.\n",
    "        \n",
    "        :returns: vector de parámetros\n",
    "        \"\"\"\n",
    "        ## TODO\n",
    "        #print(\"thetha0\", self.Theta_0, self.Theta_0.ravel())\n",
    "        #print(\"thetha1\", self.Theta_1, self.Theta_1.ravel())\n",
    "        return np.concatenate((self.Theta_0.ravel(), self.Theta_1.ravel()))\n",
    "    \n",
    "    def reconstruct_matrices(self, vector):\n",
    "        \"\"\"\n",
    "        Dado un vector, rearma matrices del tamaño de las matrices de sesgos y pesos.\n",
    "        \n",
    "        :returns: matrices de parámetros\n",
    "        \"\"\"\n",
    "        # Reshape and index to get weight matrices\n",
    "        n_inputs = self.Theta_0.shape[0]\n",
    "        n_hidden = self.Theta_0.shape[1]\n",
    "        n_outputs = self.Theta_1.shape[1]\n",
    "        print()\n",
    "        T0 = vector[:((n_inputs ) * n_hidden)].reshape(n_inputs, n_hidden)\n",
    "        T1 = vector[((n_inputs ) * n_hidden):].reshape(n_hidden , n_outputs)\n",
    "        \n",
    "        return T0, T1\n",
    "\n",
    "\n",
    "        \n",
    "    def approx_gradient(self, X, Y, lambda_r = 0.0):\n",
    "        \"\"\"\n",
    "        Aproxima el valor del gradiente alrededor de los pesos actuales,\n",
    "        perturbando cada peso, uno por uno hasta estimar la variación alrededor\n",
    "        de cada peso.\n",
    "        \n",
    "        En este método se itera sobre cada peso w:\n",
    "        * Sea w - epsilon -> val1, se calcula el error e1 cometido por la red si w es\n",
    "                                   reemplazado por val1.\n",
    "        * Sea w + epsilon -> val2, se calcula el error e2 cometido por la red si w es\n",
    "                                   reemplazado por val2.\n",
    "        * La parcial correspondiente se estima como (val1 - val2)/(2 * epsilon)\n",
    "        \n",
    "        Este método sólo se utiliza para verificar que backpropagation esté bien\n",
    "        implementado, ya que en la práctica es muy lento y menos preciso.\n",
    "        \n",
    "        :returns: matrices que tienen la misma forma de Theta_0 y Theta_1, donde\n",
    "                  cada entrada es la estimación de la parcial del error con\n",
    "                  respecto al peso correspondiente\n",
    "        \"\"\"\n",
    "        ## TODO\n",
    "            # Inicializar las matrices de gradiente\n",
    "        Theta_0_grad = np.zeros(self.Theta_0.shape)\n",
    "        Theta_1_grad = np.zeros(self.Theta_1.shape)\n",
    "    \n",
    "        # Definir el valor de epsilon\n",
    "        epsilon = 1e-4\n",
    "    \n",
    "        # Iterar sobre los pesos de Theta_0\n",
    "        for i in range(self.Theta_0.shape[0]):\n",
    "            for j in range(self.Theta_0.shape[1]):\n",
    "                theta_temp = np.copy(self.Theta_0)\n",
    "            \n",
    "                \n",
    "                theta_temp[i,j] = theta_temp[i,j] - epsilon\n",
    "                A1, A2 = self.forward_propagation(X, theta_temp, self.Theta_1)\n",
    "                cost1 = self.calc_error(A2, Y, theta_temp, self.Theta_1, lambda_r)\n",
    "                theta_temp = np.copy(self.Theta_0)\n",
    "                theta_temp[i,j] = theta_temp[i,j] + epsilon\n",
    "                A1, A2 = self.forward_propagation(X, theta_temp, self.Theta_1)\n",
    "                cost2 = self.calc_error(A2, Y, theta_temp, self.Theta_1, lambda_r)\n",
    "                Theta_0_grad[i,j] = (cost1 - cost2) / (2 * epsilon)\n",
    "    \n",
    "        # Iterar sobre los pesos de Theta_1\n",
    "        for i in range(self.Theta_1.shape[0]):\n",
    "            for j in range(self.Theta_1.shape[1]):\n",
    "                theta_temp = np.copy(self.Theta_1)\n",
    "                theta_temp[i,j] = theta_temp[i,j] - epsilon\n",
    "                A1, A2 = self.forward_propagation(X, self.Theta_0, theta_temp)\n",
    "                cost1 = self.calc_error(A2, Y, self.Theta_0, theta_temp, lambda_r=lambda_r)\n",
    "                theta_temp = np.copy(self.Theta_1)\n",
    "                theta_temp[i,j] = theta_temp[i,j] + epsilon\n",
    "                A1, A2 = self.forward_propagation(X, self.Theta_0, theta_temp)\n",
    "                cost2 = self.calc_error(A2, Y, self.Theta_0, theta_temp, lambda_r)\n",
    "                Theta_1_grad[i,j] = (cost1 - cost2) / (2 * epsilon)\n",
    "        return Theta_0_grad,Theta_1_grad\n",
    "        \n",
    "    def gradient_descent(self, X, Y, alpha, ciclos=10, check_gradient = False, lambda_r = 0.0):\n",
    "        \"\"\" Evalúa y ajusta los pesos de la red, de acuerdo a los datos en X y los resultados\n",
    "        deseados en Y.  Al final grafica el error vs ciclo.  Si el entrenamiento es correcto\n",
    "        el error debe descender por cada iteración (ciclo).\n",
    "        \n",
    "        :param X: datos de entrada\n",
    "        :param Y: salidas deseadas\n",
    "        :param alpha: taza de aprendizaje\n",
    "        :param ciclos: número de veces que se realizarán ajustes para todo el conjunto de datos X\n",
    "        :param check_gradient: se calculará el gradiente con backpropagation y con aproximación por\n",
    "                               perturbaciones, imprimiendo los valores lado a lado para que puedan\n",
    "                               ser comparados.\n",
    "        :param lambda_r: coeficiente de regularización\n",
    "        \"\"\"\n",
    "        ## TODO\n",
    "        error_por_ciclo = np.zeros(ciclos)\n",
    "\n",
    "        for i in range(ciclos):\n",
    "            self.feed_forward(X)\n",
    "\n",
    "            # Calculamos el error\n",
    "            error = self.calc_error(self.A2, Y, lambda_r=lambda_r)\n",
    "\n",
    "            # Calculamos el gradiente usando backpropagation\n",
    "            self.back_propagate(X,Y,lambda_r=lambda_r)\n",
    "            # Si se desea, se calcula el gradiente mediante aproximación por perturbaciones\n",
    "            if check_gradient:\n",
    "                approx_grad1, approx_grad2 = self.approx_gradient(X, Y, lambda_r=lambda_r)\n",
    "                print(\"Gradiente con backpropagation:\\n\")\n",
    "                print(self.D1)\n",
    "                print(self.D2)\n",
    "                print(\"\\nGradiente con aproximación por perturbaciones:\\n\")\n",
    "                print(approx_grad1)\n",
    "                print(approx_grad2)\n",
    "            self.update_weights(self.D1, self.D2,alpha)\n",
    "            error_por_ciclo[i] = error\n",
    "        plt.plot(error_por_ciclo)\n",
    "        plt.title(\"Error vs ciclo\")\n",
    "        plt.xlabel(\"Ciclo\")\n",
    "        plt.ylabel(\"Error\")\n",
    "        plt.show()\n",
    "\n",
    "    def update_weights(self,d0,d1,alpha):\n",
    "        \"\"\"\n",
    "        Función auxiliar para actualizar los pesos\n",
    "        \"\"\"\n",
    "        self.Theta_0=self.Theta_0-alpha*d0\n",
    "        self.Theta_1=self.Theta_1-alpha*d1\n",
    "        \n",
    "    def print_output(self):\n",
    "        \"\"\"\n",
    "        Muestra en pantalla los valores de salida obtenidos en la última ejecución de feed_forward.\n",
    "        \"\"\"\n",
    "        ## TODO\n",
    "        print(self.A2)\n",
    "        \n",
    "    \n",
    "    def confusion_matrix(Y):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjunto de datos\n",
    "\n",
    "Probaremos esta red con la función más sencilla que requiere más de un perceptrón: $XOR$.\n",
    "\n",
    "Para ello requerimos una red con la arquitectura siguiente:\n",
    "\n",
    "<img src=\"figuras/xor_sin_pesos.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta0-2 [[ 0.26551159  0.10854853]\n",
      " [ 0.00429143 -0.17460021]\n",
      " [ 0.43302619  1.20303737]]\n",
      "[[0.43473481]\n",
      " [0.47546745]\n",
      " [0.4325518 ]\n",
      " [0.47396381]]\n"
     ]
    }
   ],
   "source": [
    "# Datos\n",
    "X = np.array([[0, 0], \n",
    "              [0, 1], \n",
    "              [1, 0],\n",
    "              [1, 1]])  # Entradas\n",
    "Y = np.array([[0], [1], [1], [0]])              # Salidas deseadas\n",
    "\n",
    "# Instanciación de la red\n",
    "xor = Multicapa(2, 2, 1)\n",
    "\n",
    "# Probamos qué valores calcula con una inicialización aleatoria\n",
    "xor.feed_forward(X)\n",
    "xor.print_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para verificar si la red funciona para lo que fue diseñada, no basta con reducir el error,\n",
    "debemos evaluar si calcula la función que queremos que calcule.  En este caso, queremos\n",
    "usarla para evaluar una función binaria.  Si realizamos un redondeo, veamos cuántas respuestas\n",
    "calcula correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_correct(red, X, Y):\n",
    "    \"\"\" Conciderando que las etiquetas en Y son binarias (0 y 1)\n",
    "    la salida h de la red se tomará como 0 si h < 0.5\n",
    "    y como 1 si h >= 0.5\n",
    "    \"\"\"\n",
    "    red.feed_forward(X)\n",
    "    H = red.A2                     # Ojo: dependiendo de tu implementación, podrías no querer la .T\n",
    "    H = np.rint(H)\n",
    "    print(\"H = \", H)\n",
    "    print(\"Y = \", Y)\n",
    "    c = np.count_nonzero((H - Y)==0)\n",
    "    print(\"Se calcularon correctamente \", c, \"entradas.\")\n",
    "    \n",
    "    return H\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta0-2 [[-0.47614201  1.30847308]\n",
      " [ 0.19501328  0.40020999]\n",
      " [-0.33763234  1.25647226]]\n",
      "H =  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Y =  [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "Se calcularon correctamente  2 entradas.\n"
     ]
    }
   ],
   "source": [
    "H = count_correct(xor, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para dar una idea general del comportamiento de la matriz, para ello creamos la matriz de confusion\n",
    "la cual muestra los errores de tipo uno asi como de tipo dos en el siguiente formato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matriz_confusion_xor(H, Y):\n",
    "    \n",
    "    \"\"\" \n",
    "    :param_H = Salida predecida por la red\n",
    "    :param_Y = Salida esperada \n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\"\n",
    "    Donde las predicciones correctas son las que se encuentran en la \n",
    "    diagonal, igualmente con las etuquetas de la tabla, se obtiene que predijo y \n",
    "    lo esperado, \n",
    "    \n",
    "    \n",
    "    m[1][0]: falso positivo -> Error del tipo 1\n",
    "    m[0][1]: falso negativo -> Error del tipo 2 \n",
    "    \n",
    "    dada la compuera XOR, se eligio unicamente hacerlo con una secuencia de \n",
    "    condicionales en lugar de ciclos, ya que no tenemos mas opciones de salia.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = [[0,0],\n",
    "         [0,0]]\n",
    "    \n",
    "    for i in range(Y.shape[0]):\n",
    "        if Y[i] == 0:\n",
    "            if H[i] == 0:\n",
    "                m[0][0] += 1\n",
    "            else:\n",
    "                m[0][1] += 1\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            if H[i] == 1:\n",
    "                m[1][1] += 1\n",
    "            else:\n",
    "                m[1][0] += 1\n",
    "                \n",
    "        \n",
    "            \n",
    "    \n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  2  0\n",
       "1  2  0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_records(matriz_confusion_xor(H,Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder implementar el algoritmo de optimización, las variables a optimizar, es decir, los pesos, deben ser colocados sobre un vector (o matriz columna, en este caso).  Es necesario convertir las matrices de pesos a vector y, para usar la red, revertir el proceso, reconstruyendo las matrices a partir del vector con los pesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores actuales de los pesos\n",
      "Theta_0:\n",
      " [[ 0.15701498  1.35878914]\n",
      " [ 0.51534525  2.38412462]\n",
      " [ 0.52919027 -0.89614384]] \n",
      "\n",
      "Theta_1:\n",
      " [[ 0.5508447 ]\n",
      " [-0.19152205]]\n",
      "\n",
      "Matrices a vector de pesos: \n",
      "[ 0.15701498  1.35878914  0.51534525  2.38412462  0.52919027 -0.89614384\n",
      "  0.5508447  -0.19152205]\n",
      "\n",
      "Reconstrucción de las matrices a partir del vector de pesos: \n",
      "\n",
      "[[ 0.15701498  1.35878914]\n",
      " [ 0.51534525  2.38412462]\n",
      " [ 0.52919027 -0.89614384]] [[ 0.5508447 ]\n",
      " [-0.19152205]]\n"
     ]
    }
   ],
   "source": [
    "# Con esta casilla verifica que tus conversiones sean correctas\n",
    "\n",
    "print(\"Valores actuales de los pesos\")\n",
    "\n",
    "print(\"Theta_0:\\n\", xor.Theta_0, \"\\n\\nTheta_1:\\n\", xor.Theta_1)\n",
    "\n",
    "print(\"\\nMatrices a vector de pesos: \")\n",
    "print(xor.vector_weights())\n",
    "\n",
    "print(\"\\nReconstrucción de las matrices a partir del vector de pesos: \")\n",
    "T0, T1 = xor.reconstruct_matrices(xor.vector_weights())\n",
    "print(T0, T1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta0-2 [[ 0.6767082   0.06072782]\n",
      " [-0.02304119 -0.55331843]\n",
      " [-1.10612324 -0.21713489]]\n",
      "Theta0-2 [[-1.41575321  0.09470831]\n",
      " [ 0.19380061  0.54323509]]\n",
      "a2 de error [[1.         0.50027957]\n",
      " [1.         0.50020298]\n",
      " [1.         0.50007143]\n",
      " [1.         0.49999727]]\n",
      "original A2 [[0.50027957]\n",
      " [0.50020298]\n",
      " [0.50007143]\n",
      " [0.49999727]]\n",
      "==============================================================\n",
      "-0.25\n",
      "==============================================================\n",
      "Theta0-2 [[ 1.10950523 -0.99263248]\n",
      " [-0.68234147 -0.4092692 ]\n",
      " [-0.24747527  1.52785112]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4949/842624618.py:151: RuntimeWarning: divide by zero encountered in log\n",
      "  J=(-1/m)*np.sum(Y*np.log(myA2) + (1-Y)*np.log(1-myA2))\n",
      "/tmp/ipykernel_4949/842624618.py:151: RuntimeWarning: invalid value encountered in multiply\n",
      "  J=(-1/m)*np.sum(Y*np.log(myA2) + (1-Y)*np.log(1-myA2))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,1) (2,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Con esta casilla revisa que backpropagation y aproximación por perturbaciones\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# den resultados semejantes.  Observa que se ejecuta sobre un solo ciclo pues es lento.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m xor\u001b[39m.\u001b[39;49mgradient_descent(X, Y, \u001b[39m0.3\u001b[39;49m, \u001b[39m1\u001b[39;49m, check_gradient \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn[65], line 268\u001b[0m, in \u001b[0;36mMulticapa.gradient_descent\u001b[0;34m(self, X, Y, alpha, ciclos, check_gradient, lambda_r)\u001b[0m\n\u001b[1;32m    265\u001b[0m error \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalc_error(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mA2, Y, lambda_r\u001b[39m=\u001b[39mlambda_r)\n\u001b[1;32m    267\u001b[0m \u001b[39m# Calculamos el gradiente usando backpropagation\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mback_propagate(X,Y,lambda_r\u001b[39m=\u001b[39;49mlambda_r)\n\u001b[1;32m    269\u001b[0m \u001b[39m# Si se desea, se calcula el gradiente mediante aproximación por perturbaciones\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[39mif\u001b[39;00m check_gradient:\n",
      "Cell \u001b[0;32mIn[65], line 113\u001b[0m, in \u001b[0;36mMulticapa.back_propagate\u001b[0;34m(self, X, Y, lambda_r)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39m# Potenciales de la capa oculta, incluyendo el sesgo\u001b[39;00m\n\u001b[1;32m    111\u001b[0m Z1_with_bias \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39minsert(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mZ1, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 113\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mGrad_1 \u001b[39m=\u001b[39m Z1_with_bias\u001b[39m.\u001b[39;49mT \u001b[39m@\u001b[39;49m delta2 \u001b[39m+\u001b[39;49m lambda_r \u001b[39m*\u001b[39;49m np\u001b[39m.\u001b[39;49minsert(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mTheta_1[\u001b[39m1\u001b[39;49m:], \u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m    115\u001b[0m \u001b[39m# Error en la capa oculta\u001b[39;00m\n\u001b[1;32m    116\u001b[0m delta1 \u001b[39m=\u001b[39m delta2 \u001b[39m@\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mTheta_1\u001b[39m.\u001b[39mT \u001b[39m*\u001b[39m derivada_logistica_atajo(Z1_with_bias)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,1) (2,1) "
     ]
    }
   ],
   "source": [
    "# Con esta casilla revisa que backpropagation y aproximación por perturbaciones\n",
    "# den resultados semejantes.  Observa que se ejecuta sobre un solo ciclo pues es lento.\n",
    "\n",
    "xor.gradient_descent(X, Y, 0.3, 1, check_gradient = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f080739ad3c45b2814f0dff7a2530d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1025, description='ciclos', max=2000, min=50), Button(description='Run I…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact_manual(ciclos = (50, 2000))\n",
    "def train_XOR(ciclos):\n",
    "    xor.gradient_descent(X, Y, 0.3, ciclos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta0-2 [[-0.52929608  1.04618286]\n",
      " [-1.41855603 -0.36249918]\n",
      " [-0.12190569  0.31935642]]\n",
      "[[0.75266342]\n",
      " [0.76402969]\n",
      " [0.74862142]\n",
      " [0.76140171]]\n",
      "\n",
      "Theta_0 =  [[-0.52929608  1.04618286]\n",
      " [-1.41855603 -0.36249918]\n",
      " [-0.12190569  0.31935642]] \n",
      "\n",
      "Theta_1 [[ 0.62133597]\n",
      " [-0.72008556]]\n"
     ]
    }
   ],
   "source": [
    "# Después de haber sido correctamente entrenada, la red debe producir salidas\n",
    "# cercanas a las deseadas.  Dado que la función de activación es la sigmoide\n",
    "# nunca llegará a 0 o 1 exactamente.\n",
    "\n",
    "xor.feed_forward(X)\n",
    "xor.print_output()\n",
    "\n",
    "print(\"\\nTheta_0 = \", xor.Theta_0, \"\\n\\nTheta_1\", xor.Theta_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta0-2 [[ 0.31475378  2.46765106]\n",
      " [-1.50832149  0.62060066]\n",
      " [-1.04513254 -0.79800882]]\n",
      "H =  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Y =  [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "Se calcularon correctamente  2 entradas.\n"
     ]
    }
   ],
   "source": [
    "c = count_correct(xor, X, Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xor_reg = Multicapa(2, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta0-2 [[-1.99439377  1.10770823]\n",
      " [ 0.24454398 -0.06191203]\n",
      " [-0.75389296  0.71195902]]\n",
      "H =  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Y =  [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "Se calcularon correctamente  2 entradas.\n"
     ]
    }
   ],
   "source": [
    "c = count_correct(xor_reg, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta0-2 [[-1.85618548 -0.2227737 ]\n",
      " [-0.06584785 -2.13171211]\n",
      " [-0.04883051  0.39334122]]\n",
      "Theta0-2 [[ 0.24454398 -0.06191203]\n",
      " [-0.75389296  0.71195902]]\n",
      "a2 de error [[1.         0.68047041]\n",
      " [1.         0.68111298]\n",
      " [1.         0.67831673]\n",
      " [1.         0.67861215]]\n",
      "original A2 [[0.68047041]\n",
      " [0.68111298]\n",
      " [0.67831673]\n",
      " [0.67861215]]\n",
      "Theta0-2 [[ 0.82699862 -1.95451212]\n",
      " [ 0.11747566 -1.90745689]\n",
      " [-0.92290926  0.46975143]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17092/1317291660.py:146: RuntimeWarning: divide by zero encountered in log\n",
      "  J=(-1/m)*np.sum(Y*np.log(myA2) + (1-Y)*np.log(1-myA2))\n",
      "/tmp/ipykernel_17092/1317291660.py:146: RuntimeWarning: invalid value encountered in multiply\n",
      "  J=(-1/m)*np.sum(Y*np.log(myA2) + (1-Y)*np.log(1-myA2))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,1) (2,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[165], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mxor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_gradient\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_r\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.15\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[153], line 263\u001b[0m, in \u001b[0;36mMulticapa.gradient_descent\u001b[0;34m(self, X, Y, alpha, ciclos, check_gradient, lambda_r)\u001b[0m\n\u001b[1;32m    260\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalc_error(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA2, Y, lambda_r\u001b[38;5;241m=\u001b[39mlambda_r)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# Calculamos el gradiente usando backpropagation\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mback_propagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlambda_r\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_r\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# Si se desea, se calcula el gradiente mediante aproximación por perturbaciones\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_gradient:\n",
      "Cell \u001b[0;32mIn[153], line 113\u001b[0m, in \u001b[0;36mMulticapa.back_propagate\u001b[0;34m(self, X, Y, lambda_r)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Potenciales de la capa oculta, incluyendo el sesgo\u001b[39;00m\n\u001b[1;32m    111\u001b[0m Z1_with_bias \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mZ1, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGrad_1 \u001b[38;5;241m=\u001b[39m \u001b[43mZ1_with_bias\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdelta2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlambda_r\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTheta_1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Error en la capa oculta\u001b[39;00m\n\u001b[1;32m    116\u001b[0m delta1 \u001b[38;5;241m=\u001b[39m delta2 \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTheta_1\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m*\u001b[39m derivada_logistica_atajo(Z1_with_bias)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,1) (2,1) "
     ]
    }
   ],
   "source": [
    "xor.gradient_descent(X, Y, 0.3, 1, check_gradient = True, lambda_r = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ed12bf08814fc1ae8267de706424e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1025, description='ciclos', max=2000, min=50), Button(description='Run I…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact_manual(ciclos = (50, 2000))\n",
    "def train_XOR_reg(ciclos):\n",
    "    # Prueba diferentes valores de lambda_r ¿qué tanto lo puedes incrementar si matar a la red?\n",
    "    xor_reg.gradient_descent(X, Y, 0.3, ciclos, lambda_r = 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta0-2 [[ 0.84820861  0.70683045]\n",
      " [-0.78726893  0.29294072]\n",
      " [-0.47080725  2.40432561]]\n",
      "[[0.23289374]\n",
      " [0.22115581]\n",
      " [0.23947263]\n",
      " [0.23115213]]\n",
      "Theta_0 =  [[ 0.84820861  0.70683045]\n",
      " [-0.78726893  0.29294072]\n",
      " [-0.47080725  2.40432561]] \n",
      "Theta_1 [[ 0.4609029 ]\n",
      " [-0.21578989]]\n"
     ]
    }
   ],
   "source": [
    "xor_reg.feed_forward(X)\n",
    "xor_reg.print_output()\n",
    "print(\"Theta_0 = \", xor_reg.Theta_0, \"\\nTheta_1\", xor_reg.Theta_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta0-2 [[-0.43902624  0.14110417]\n",
      " [ 0.27304932 -1.61857075]\n",
      " [-0.57311336 -1.32044755]]\n",
      "H =  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Y =  [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "Se calcularon correctamente  2 entradas.\n"
     ]
    }
   ],
   "source": [
    "c = count_correct(xor_reg, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    img {\n",
       "        display: block;\n",
       "\t\tmargin-left: auto;\n",
       "\t\tmargin-right: auto;\n",
       "\t}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"styles/custom.css\", \"r\").read() #or edit path to custom.css\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
